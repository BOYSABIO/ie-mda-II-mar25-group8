{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22dc1ac9-8e1d-4e7c-9202-68b530c889a4",
   "metadata": {},
   "source": [
    "# Creating Fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0f3840-d674-4a54-9859-57fae06367ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30924529-f44e-4fc0-842c-01f89ab180b5",
   "metadata": {},
   "source": [
    "I initially wanted to stream the binary audio files into kafka and then transform them into fingerprints so I wouldn't have had to download them onto disk. However, kafka has a limitation at 1MB for streaming files and the audio files were well over that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c7c0b-cc0d-4b7a-b4c9-9d62d840200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.2\" pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2759b6-a40e-4d99-9a1f-6e0ecacb9edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MP3 Binary Kafka Producer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define your HDFS silver layer path (where binary mp3 Parquet is stored)\n",
    "input_hdfs_path = \"hdfs://localhost:9000/lakehouse/silver/mp3_binary/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c42f18-609f-4745-ac35-994b4a3c8005",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "binary_df = spark.read.parquet(input_hdfs_path)\n",
    "binary_df.printSchema()\n",
    "binary_df.select(\"filename\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47d7e8d-a06e-4c2a-8b64-d5ca7c4df714",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_ready_df = binary_df.selectExpr(\n",
    "    \"CAST(filename AS STRING) AS key\",\n",
    "    \"CAST(content AS BINARY) AS value\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9621b96f-3327-4f50-af78-20e8c1f5088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_ready_df.write \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"topic\", \"mp3_binary_stream\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104353ee-f63e-4ea4-9467-6e6900e28d22",
   "metadata": {},
   "source": [
    "Code for Download on disk and conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b484f994-4853-436a-9063-516a8393931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "import os\n",
    "import tempfile\n",
    "import librosa\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be75580-4f6e-4f3f-9d8a-406988b86314",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Audio Fingerprint Extraction\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125cf53f-c70d-4e13-ba59-d9726825ba8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_input_path = \"hdfs://localhost:9000/lakehouse/silver/mp3_binary/\"\n",
    "hdfs_output_path = \"hdfs://localhost:9000/warehouse/fingerprints_parquet/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ed7221-22db-44c4-95fd-bcf9bb9e743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_input_dir = tempfile.mkdtemp(prefix=\"mp3_input_\")\n",
    "print(f\"[INFO] Temporary local input folder created at: {temp_input_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545c9c46-b738-4bff-9751-12757f067512",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_df = spark.read.parquet(hdfs_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b00e0f0-a69b-4120-8808-6a40ce766401",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = binary_df.select(\"filename\", \"content\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf1dcc0-1d0d-4e35-862b-41a998b0668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "fingerprint_entries = []\n",
    "\n",
    "for row in records:\n",
    "    filename = row[\"filename\"]\n",
    "    content = row[\"content\"]\n",
    "\n",
    "    try:\n",
    "        # Save MP3 from binary\n",
    "        tmp_mp3_path = os.path.join(tempfile.gettempdir(), filename)\n",
    "        with open(tmp_mp3_path, \"wb\") as f:\n",
    "            f.write(content)\n",
    "\n",
    "        # Extract fingerprint\n",
    "        y, sr = librosa.load(tmp_mp3_path, sr=44100)\n",
    "        D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
    "        peaks = np.argwhere(D > np.percentile(D, 95)).tolist()\n",
    "\n",
    "        fingerprint_entries.append((filename, peaks, str(datetime.now())))\n",
    "\n",
    "    except Exception as e:\n",
    "        fingerprint_entries.append((filename, None, f\"ERROR: {e}\"))\n",
    "\n",
    "# Create DataFrame from results\n",
    "fingerprint_df = spark.createDataFrame(fingerprint_rdd, schema=[\"filename\", \"fingerprint\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8e946a-eb6b-4491-83df-45e993637042",
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprint_df.repartition(10).write.mode(\"overwrite\").parquet(hdfs_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147d4193-4adf-4e81-a75d-879eb652c938",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65528168-e237-4e3e-bdca-dc51de39c4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, ArrayType, IntegerType\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2d5f450-1d7e-43d7-aa02-3c31d60c7977",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"MP3 Fingerprint Extraction\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db65f1bc-8603-4f9d-896a-bfd4a996ebac",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp3_input_dir = \"songs_mp3/\"\n",
    "hdfs_output_path = \"hdfs://localhost:9000/warehouse/fingerprints/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db795f27-0f76-49a3-89b5-123c04c98f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fingerprint(file_path):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=44100)\n",
    "        D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
    "        peaks = np.argwhere(D > np.percentile(D, 95))  # Top 5% peaks\n",
    "        flattened = [f\"{row}-{col}\" for row, col in peaks]  # Flatten into strings\n",
    "        return Row(\n",
    "            filename=os.path.basename(file_path),\n",
    "            fingerprint=flattened,\n",
    "            status=\"Success\",\n",
    "            message=\"Flattened fingerprint generated\",\n",
    "            timestamp=str(datetime.now())\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return Row(\n",
    "            filename=os.path.basename(file_path),\n",
    "            fingerprint=[],\n",
    "            status=\"Error\",\n",
    "            message=str(e),\n",
    "            timestamp=str(datetime.now())\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c017c3a-a598-4889-b1d3-6bd569b79406",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"filename\", StringType(), True),\n",
    "    StructField(\"fingerprint\", ArrayType(StringType()), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"message\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "329c7c63-9264-4780-ac22-2f448d64f0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp3_files = [os.path.join(mp3_input_dir, f) for f in os.listdir(mp3_input_dir) if f.endswith(\".mp3\")]\n",
    "fingerprint_rdd = spark.sparkContext.parallelize(mp3_files).map(extract_fingerprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11479c48-ea4c-45eb-8fc2-aae5f85457fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprint_df = spark.createDataFrame(fingerprint_rdd, schema=schema)\n",
    "fingerprint_df.write.mode(\"overwrite\").parquet(hdfs_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d549967-4332-440f-95de-96b84d57386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_check = spark.read.parquet(hdfs_output_path)\n",
    "\n",
    "# Reconstruct peaks into row/col struct\n",
    "df_with_peaks = df_check.withColumn(\n",
    "    \"reconstructed_peaks\",\n",
    "    F.expr(\"transform(fingerprint, x -> struct(cast(split(x, '-')[0] as int) as row, cast(split(x, '-')[1] as int) as col))\")\n",
    ")\n",
    "\n",
    "df_with_peaks.select(\"filename\", \"reconstructed_peaks\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95ba1fa-f2cd-4dd8-a641-ada55fcc0e78",
   "metadata": {},
   "source": [
    "Writing to Parquet Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7b047d3-f7c8-412a-8dbc-26d91fb528c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/16 21:10:55 WARN Utils: Your hostname, osbdet resolves to a loopback address: 127.0.0.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/03/16 21:10:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/16 21:11:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/16 21:11:13 WARN TaskSetManager: Stage 0 contains a task of very large size (4106 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"Convert Fingerprints to Parquet\").getOrCreate()\n",
    "\n",
    "# Paths\n",
    "fingerprint_folder = \"fingerprints\"\n",
    "local_parquet_output = \"parquet\"\n",
    "\n",
    "# Prepare data\n",
    "fingerprint_data = []\n",
    "\n",
    "for file in os.listdir(fingerprint_folder):\n",
    "    if file.endswith(\".npy\"):\n",
    "        try:\n",
    "            filepath = os.path.join(fingerprint_folder, file)\n",
    "            peaks = np.load(filepath, allow_pickle=True)\n",
    "\n",
    "            fingerprint_data.append(Row(\n",
    "                filename=file.replace(\".npy\", \".mp3\"),\n",
    "                fingerprint=peaks.tolist(),\n",
    "                status=\"Success\",\n",
    "                message=\"Converted from .npy\",\n",
    "                timestamp=str(datetime.now())\n",
    "            ))\n",
    "        except Exception as e:\n",
    "            fingerprint_data.append(Row(\n",
    "                filename=file.replace(\".npy\", \".mp3\"),\n",
    "                fingerprint=[],\n",
    "                status=\"Error\",\n",
    "                message=str(e),\n",
    "                timestamp=str(datetime.now())\n",
    "            ))\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"filename\", StringType(), True),\n",
    "    StructField(\"fingerprint\", ArrayType(ArrayType(IntegerType())), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"message\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "fingerprint_df = spark.createDataFrame(fingerprint_data, schema=schema)\n",
    "\n",
    "# Write to local Parquet\n",
    "fingerprint_df.write.mode(\"overwrite\").parquet(local_parquet_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674eb8ac-f608-4384-93a0-74b8ed5c24dd",
   "metadata": {},
   "source": [
    "Example of Writing it to Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3533b1f8-a0a8-4346-bf92-9f7b8bf40cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/16 21:15:01 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "25/03/16 21:15:02 WARN TaskSetManager: Stage 1 contains a task of very large size (4106 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"Convert Fingerprints to Parquet\").getOrCreate()\n",
    "\n",
    "# Paths\n",
    "fingerprint_folder = \"fingerprints\"\n",
    "local_parquet_output = \"parquet\"\n",
    "\n",
    "# Prepare data\n",
    "fingerprint_data = []\n",
    "\n",
    "for file in os.listdir(fingerprint_folder):\n",
    "    if file.endswith(\".npy\"):\n",
    "        try:\n",
    "            filepath = os.path.join(fingerprint_folder, file)\n",
    "            peaks = np.load(filepath, allow_pickle=True)\n",
    "\n",
    "            fingerprint_data.append(Row(\n",
    "                filename=file.replace(\".npy\", \".mp3\"),\n",
    "                fingerprint=peaks.tolist(),\n",
    "                status=\"Success\",\n",
    "                message=\"Converted from .npy\",\n",
    "                timestamp=str(datetime.now())\n",
    "            ))\n",
    "        except Exception as e:\n",
    "            fingerprint_data.append(Row(\n",
    "                filename=file.replace(\".npy\", \".mp3\"),\n",
    "                fingerprint=[],\n",
    "                status=\"Error\",\n",
    "                message=str(e),\n",
    "                timestamp=str(datetime.now())\n",
    "            ))\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"filename\", StringType(), True),\n",
    "    StructField(\"fingerprint\", ArrayType(ArrayType(IntegerType())), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"message\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "fingerprint_df = spark.createDataFrame(fingerprint_data, schema=schema)\n",
    "\n",
    "# Write to local Parquet\n",
    "fingerprint_df.write.mode(\"overwrite\").parquet(\"hdfs://localhost:9000/warehouse/fingerprints_parquet/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
